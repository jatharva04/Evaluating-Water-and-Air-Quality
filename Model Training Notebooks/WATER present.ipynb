{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k4dG2-ycm54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e366e4a-f118-492a-b5b9-c8f90a468967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_FILE = \"/content/drive/MyDrive/cur_water_data_labeled.xlsx\"\n",
        "OUTPUT_FILE = \"cur_water_data_with_month_numbers.xlsx\"\n",
        "DATE_COLUMN = \"sample_date\"\n",
        "\n",
        "def get_season(month_number):\n",
        "    \"\"\"\n",
        "    Categorizes a month number into a season based on the provided logic.\n",
        "    \"\"\"\n",
        "    if month_number in [3, 4, 5, 6]:\n",
        "        return \"Summer\"\n",
        "    elif month_number in [7, 8, 9, 10]:\n",
        "        return \"Monsoon\"\n",
        "    else:  # Covers months 11, 12, 1, 2\n",
        "        return \"Winter\"\n",
        "\n",
        "# --- Main Script ---\n",
        "try:\n",
        "    # 1. Load the dataset\n",
        "    print(f\"Reading data from '{INPUT_FILE}'...\")\n",
        "    df = pd.read_excel('/content/drive/MyDrive/cur_water_data_labeled.xlsx', engine='openpyxl')\n",
        "    print(\"‚úÖ Data loaded successfully.\")\n",
        "\n",
        "    # 2. Convert the date column to a proper datetime format\n",
        "    df['date'] = pd.to_datetime(df[DATE_COLUMN], errors='coerce')\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "    print(f\"‚úÖ Converted '{DATE_COLUMN}' to a usable date format.\")\n",
        "\n",
        "    # 3. Create the new 'Year', 'Month', and 'Season' columns\n",
        "    df['Year'] = df['date'].dt.year\n",
        "    df['Month'] = df['date'].dt.month # Get the month number\n",
        "    df['Season'] = df['Month'].apply(get_season) # Use the month number for logic\n",
        "\n",
        "    print(\"‚úÖ New 'Year', 'Month' (as a number), and 'Season' columns created.\")\n",
        "\n",
        "    # 4. Show a preview of the new columns\n",
        "    print(\"\\n--- Preview of the new data ---\")\n",
        "    print(df[[DATE_COLUMN, 'Year', 'Month', 'Season']].head())\n",
        "\n",
        "    # 5. Save the updated DataFrame to a new Excel file\n",
        "    # We drop the temporary 'date' column before saving\n",
        "    df.drop(columns=['date'], inplace=True)\n",
        "    df.to_excel(OUTPUT_FILE, index=False)\n",
        "    print(f\"\\n‚úÖ Success! New file saved as '{OUTPUT_FILE}'\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Input file '{INPUT_FILE}' not found. Please make sure it's in the same folder.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "TJHtjaLuYy-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a04499d-efbf-4381-f0f0-85c8e5b68570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data from '/content/drive/MyDrive/cur_water_data_labeled.xlsx'...\n",
            "‚ùå Error: Input file '/content/drive/MyDrive/cur_water_data_labeled.xlsx' not found. Please make sure it's in the same folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall flask-ngrok -y\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "1JrN3dgMHS7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ebc1ad-e041-4da9-903f-d880cb2ceec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping flask-ngrok as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas openpyxl"
      ],
      "metadata": {
        "id": "hZaZ8CfiOw7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984666f0-ab4f-4f1f-e08f-8b633a03673c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# --- Configuration ---\n",
        "# ‚¨áÔ∏è UPDATE THIS with the path to your Excel file ‚¨áÔ∏è\n",
        "INPUT_FILE = \"/content/drive/MyDrive/cur_water_data_with_month_numbers.xlsx\"\n",
        "# Name for the new, cleaned output file\n",
        "OUTPUT_FILE = \"water_data_cleaned.csv\"\n",
        "\n",
        "try:\n",
        "    # 1. Load the dataset from the Excel file\n",
        "    print(f\"--- Loading Data from '{INPUT_FILE}' ---\")\n",
        "    # The corrected function is pd.read_excel() and we use the 'openpyxl' engine\n",
        "    df = pd.read_excel(INPUT_FILE, engine='openpyxl')\n",
        "    print(\"‚úÖ Data loaded successfully.\")\n",
        "\n",
        "    # --- 2. Clean the Data ---\n",
        "    print(\"\\n--- Cleaning Data ---\")\n",
        "\n",
        "    # List of columns that might contain non-numeric text like '(BDL)'\n",
        "    # Add any other columns you need to clean to this list\n",
        "    cols_to_clean = [\n",
        "        'bod', 'cod', 'nitrate', 'FecalColiform',\n",
        "        # Adding other potential columns just in case names vary\n",
        "        'B.O.D. (mg/L)', 'C.O.D. (mg/L)', 'Nitrate (mg/L)', 'Fecal Coliform (MPN/100 ml)'\n",
        "    ]\n",
        "\n",
        "    cleaned_cols_found = []\n",
        "    for col in cols_to_clean:\n",
        "        if col in df.columns:\n",
        "            cleaned_cols_found.append(col)\n",
        "            # First, convert the column to a string type to handle any mixed data\n",
        "            df[col] = df[col].astype(str)\n",
        "            # Use regex to find and extract the first number (integer or float)\n",
        "            df[col] = df[col].str.extract(r'(\\d+\\.?\\d*)', expand=False)\n",
        "            # Convert the cleaned column back to a numeric type\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    if cleaned_cols_found:\n",
        "        print(f\"‚úÖ Cleaned the following columns by removing text like '(BDL)': {cleaned_cols_found}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No columns with names matching the cleaning list were found.\")\n",
        "\n",
        "    # --- 3. Show a Preview ---\n",
        "    print(\"\\n--- Preview of Cleaned Data ---\")\n",
        "    # Display the first 5 rows of the potentially cleaned columns to verify\n",
        "    if cleaned_cols_found:\n",
        "        print(df[cleaned_cols_found].head())\n",
        "    else:\n",
        "        print(\"No columns were cleaned.\")\n",
        "\n",
        "    # --- 4. Save the Cleaned Data ---\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "    print(f\"\\n‚úÖ Success! Cleaned data has been saved to '{OUTPUT_FILE}'\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Input file '{INPUT_FILE}' not found. Please check the name and path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "emCQuvxtkcJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c962732-a0bc-43a2-9a02-2b3b152d0740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading Data from '/content/drive/MyDrive/cur_water_data_with_month_numbers.xlsx' ---\n",
            "‚ùå Error: Input file '/content/drive/MyDrive/cur_water_data_with_month_numbers.xlsx' not found. Please check the name and path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas openpyxl scikit-learn"
      ],
      "metadata": {
        "id": "6Pnj4lEXQKXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0fee036-4fd3-4ecc-9a89-fbebe2cdc9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL TRAINING**"
      ],
      "metadata": {
        "id": "7dFxJUkyvedy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LOAD AND CLEAN THE DATA\n",
        "# ==============================================================================\n",
        "print(\"--- 1. Loading and Cleaning Data ---\")\n",
        "try:\n",
        "    # Use the filename provided from your upload\n",
        "    file_path = \"/content/drive/MyDrive/cur_water_data_cleaned.csv\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"‚úÖ Data loaded successfully with {len(df)} rows.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: File '{file_path}' not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- Clean all potential numeric columns ---\n",
        "# This ensures columns like 'bod', 'cod', etc., are purely numeric\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        # Extracts the first number found in a string, handles cases like '0.3 (BDL)'\n",
        "        df[col] = df[col].astype(str).str.extract(r'(\\d+\\.?\\d*)', expand=False)\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# --- Handle Missing Values using Median Imputation ---\n",
        "for column in df.select_dtypes(include=np.number).columns:\n",
        "    median_value = df[column].median()\n",
        "    df[column].fillna(median_value, inplace=True)\n",
        "print(\"‚úÖ Data cleaned and missing values handled.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. PREPARE DATA FOR THE MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 2. Preparing Data ---\")\n",
        "\n",
        "# --- Define Target (y) and Initial Features (X) ---\n",
        "TARGET_COLUMN = 'wqi'\n",
        "\n",
        "# Drop the target and columns directly derived from it to prevent data leakage\n",
        "# Also drop the original date string, as we have Year and Month\n",
        "X = df.drop(columns=[TARGET_COLUMN, 'wqi_index', 'sample_date'], errors='ignore')\n",
        "y = df[TARGET_COLUMN]\n",
        "\n",
        "# --- Handle Categorical Features using One-Hot Encoding ---\n",
        "# This converts text columns like 'Station Name' and 'Season' into a numeric format\n",
        "X = pd.get_dummies(X, columns=['Station Name', 'Season'], drop_first=True)\n",
        "\n",
        "print(f\"üéØ Target variable: '{TARGET_COLUMN}'\")\n",
        "print(f\"üìñ Final features being used ({len(X.columns)}): {X.columns.tolist()}\")\n",
        "\n",
        "# --- Split Data into Training and Testing Sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"\\n‚úÖ Data split into training and testing sets.\")\n",
        "\n",
        "# --- Scale the Features ---\n",
        "# This standardizes all numerical features to a common scale\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"‚úÖ Features scaled successfully.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TRAIN THE PREDICTIVE MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 3. Training the Model ---\")\n",
        "\n",
        "# We will use a RandomForestRegressor, which is excellent for this type of problem\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train the model on the prepared training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"‚úÖ Model trained successfully!\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. EVALUATE THE MODEL'S PERFORMANCE\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 4. Evaluating Model Performance ---\")\n",
        "\n",
        "# Make predictions on the unseen test data\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# --- Print Performance Metrics ---\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"R-squared (R¬≤): {r2:.2f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. UNDERSTAND WHAT DRIVES THE PREDICTION (FEATURE IMPORTANCE)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 5. Feature Importance ---\")\n",
        "\n",
        "# Create a DataFrame to see which features were most important to the model\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top features influencing the prediction:\")\n",
        "print(importance_df.head(10)) # Display top 10 features"
      ],
      "metadata": {
        "id": "JsmqmphbQKUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5c7c35-41b9-4f3a-95f4-7a9ae0a523be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Loading and Cleaning Data ---\n",
            "‚úÖ Data loaded successfully with 900 rows.\n",
            "‚úÖ Data cleaned and missing values handled.\n",
            "\n",
            "--- 2. Preparing Data ---\n",
            "üéØ Target variable: 'wqi'\n",
            "üìñ Final features being used (10): ['latitude', 'longitude', 'pH', 'dissolvedoxygen', 'bod', 'cod', 'nitrate', 'FecalColiform', 'Year', 'Month']\n",
            "\n",
            "‚úÖ Data split into training and testing sets.\n",
            "‚úÖ Features scaled successfully.\n",
            "\n",
            "--- 3. Training the Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3540436072.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[column].fillna(median_value, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model trained successfully!\n",
            "\n",
            "--- 4. Evaluating Model Performance ---\n",
            "Mean Absolute Error (MAE): 1.18\n",
            "R-squared (R¬≤): 0.98\n",
            "\n",
            "--- 5. Feature Importance ---\n",
            "Top features influencing the prediction:\n",
            "           Feature  Importance\n",
            "3  dissolvedoxygen    0.487083\n",
            "4              bod    0.269537\n",
            "5              cod    0.152543\n",
            "7    FecalColiform    0.060365\n",
            "2               pH    0.024013\n",
            "1        longitude    0.003263\n",
            "6          nitrate    0.001057\n",
            "9            Month    0.000911\n",
            "0         latitude    0.000645\n",
            "8             Year    0.000583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN41ziwx3vlJ",
        "outputId": "83f676d7-7b2f-4248-c517-b6036fbf4cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGboost Model used for Final Project**"
      ],
      "metadata": {
        "id": "_9zMJcvlbZrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn xgboost lightgbm"
      ],
      "metadata": {
        "id": "REI57Rc-uXqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe544e09-93a6-4ab7-d81a-859645254f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LOAD AND PREPARE THE DATA\n",
        "# ==============================================================================\n",
        "print(\"--- 1. Loading and Preparing Data for XGBoost ---\")\n",
        "try:\n",
        "    # Use the cleaned data file you provided\n",
        "    file_path = \"cur_water_data_cleaned.csv\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"‚úÖ Data loaded successfully with {len(df)} rows.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: File '{file_path}' not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- Define Target (y) and Features (X) ---\n",
        "TARGET_COLUMN = 'wqi'\n",
        "# For this model, we'll focus on the core numerical measurements\n",
        "features_to_use = [\n",
        "    'pH', 'dissolvedoxygen', 'bod', 'cod', 'nitrate', 'FecalColiform',\n",
        "    'Year', 'Month', 'latitude', 'longitude'\n",
        "]\n",
        "X = df[features_to_use].copy() # Create a copy to avoid SettingWithCopyWarning\n",
        "y = df[TARGET_COLUMN]\n",
        "\n",
        "print(f\"üéØ Target variable: '{TARGET_COLUMN}'\")\n",
        "print(f\"üìñ Features ({len(X.columns)}) being used: {X.columns.tolist()}\")\n",
        "\n",
        "# --- Clean all potential numeric columns in the selected features ---\n",
        "# Ensure columns like 'bod', 'cod', etc., are purely numeric\n",
        "for col in features_to_use:\n",
        "    if col in X.columns:\n",
        "        # Extracts the first number found in a string, handles cases like '0.3 (BDL)'\n",
        "        X[col] = X[col].astype(str).str.extract(r'(\\d+\\.?\\d*)', expand=False)\n",
        "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "\n",
        "# --- Handle Missing Values using Median Imputation ---\n",
        "for column in X.select_dtypes(include=np.number).columns:\n",
        "    median_value = X[column].median()\n",
        "    X[column].fillna(median_value, inplace=True)\n",
        "print(\"‚úÖ Data cleaned and missing values handled.\")\n",
        "\n",
        "\n",
        "# --- Split and Scale Data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "joblib.dump(X_train, \"X_train_wqi.pkl\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"‚úÖ Data prepared, split, and scaled successfully.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. TRAIN AND EVALUATE XGBOOST MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 2. Training XGBoost Model ---\")\n",
        "# Initialize and train the XGBRegressor\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "print(\"‚úÖ XGBoost model trained successfully!\")\n",
        "\n",
        "# --- Evaluate Performance ---\n",
        "print(\"\\n--- XGBoost Model Performance ---\")\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae_xgb:.2f}\")\n",
        "print(f\"R-squared (R¬≤): {r2_xgb:.2f}\")\n",
        "\n",
        "# --- Feature Importance ---\n",
        "xgb_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop features (XGBoost):\")\n",
        "print(xgb_importance_df.head())"
      ],
      "metadata": {
        "id": "kp9WAXWhuXnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d02df9-d71d-45ed-a02c-59019fe2cf53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Loading and Preparing Data for XGBoost ---\n",
            "‚úÖ Data loaded successfully with 900 rows.\n",
            "üéØ Target variable: 'wqi'\n",
            "üìñ Features (10) being used: ['pH', 'dissolvedoxygen', 'bod', 'cod', 'nitrate', 'FecalColiform', 'Year', 'Month', 'latitude', 'longitude']\n",
            "‚úÖ Data cleaned and missing values handled.\n",
            "‚úÖ Data prepared, split, and scaled successfully.\n",
            "\n",
            "--- 2. Training XGBoost Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1572078496.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[column].fillna(median_value, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ XGBoost model trained successfully!\n",
            "\n",
            "--- XGBoost Model Performance ---\n",
            "Mean Absolute Error (MAE): 0.95\n",
            "R-squared (R¬≤): 0.99\n",
            "\n",
            "Top features (XGBoost):\n",
            "           Feature  Importance\n",
            "1  dissolvedoxygen    0.480543\n",
            "2              bod    0.406799\n",
            "5    FecalColiform    0.078269\n",
            "3              cod    0.019331\n",
            "0               pH    0.011481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Bundle model and scaler\n",
        "model_bundle = {\n",
        "    \"model\": xgb_model,\n",
        "    \"scaler\": scaler,\n",
        "    \"features\": features_to_use\n",
        "}\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"xgboost_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(model_bundle, model_file)"
      ],
      "metadata": {
        "id": "T77sV8XhZ1_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LightGBM Model not used for Final Model**"
      ],
      "metadata": {
        "id": "BqsQKo5obqDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LOAD AND PREPARE THE DATA\n",
        "# ==============================================================================\n",
        "print(\"--- 1. Loading and Preparing Data for LightGBM ---\")\n",
        "try:\n",
        "    # Use the cleaned data file you provided\n",
        "    file_path = \"/content/drive/MyDrive/cur_water_data_cleaned.csv\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"‚úÖ Data loaded successfully with {len(df)} rows.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: File '{file_path}' not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- Define Target (y) and Features (X) ---\n",
        "TARGET_COLUMN = 'wqi'\n",
        "# For this model, we'll focus on the core numerical measurements\n",
        "features_to_use = [\n",
        "    'pH', 'dissolvedoxygen', 'bod', 'cod', 'nitrate', 'FecalColiform',\n",
        "    'Year', 'Month', 'latitude', 'longitude'\n",
        "]\n",
        "X = df[features_to_use].copy() # Create a copy to avoid SettingWithCopyWarning\n",
        "y = df[TARGET_COLUMN]\n",
        "\n",
        "print(f\"üéØ Target variable: '{TARGET_COLUMN}'\")\n",
        "print(f\"üìñ Features ({len(X.columns)}) being used: {X.columns.tolist()}\")\n",
        "\n",
        "# --- Clean all potential numeric columns in the selected features ---\n",
        "# Ensure columns like 'bod', 'cod', etc., are purely numeric\n",
        "for col in features_to_use:\n",
        "    if col in X.columns:\n",
        "        # Extracts the first number found in a string, handles cases like '0.3 (BDL)'\n",
        "        X[col] = X[col].astype(str).str.extract(r'(\\d+\\.?\\d*)', expand=False)\n",
        "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "\n",
        "# --- Handle Missing Values using Median Imputation ---\n",
        "for column in X.select_dtypes(include=np.number).columns:\n",
        "    median_value = X[column].median()\n",
        "    X[column].fillna(median_value, inplace=True)\n",
        "print(\"‚úÖ Data cleaned and missing values handled.\")\n",
        "\n",
        "\n",
        "# --- Split and Scale Data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"‚úÖ Data prepared, split, and scaled successfully.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. TRAIN AND EVALUATE LIGHTGBM MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\n--- 2. Training LightGBM Model ---\")\n",
        "# Initialize and train the LGBMRegressor\n",
        "lgbm_model = lgb.LGBMRegressor(random_state=42)\n",
        "lgbm_model.fit(X_train_scaled, y_train)\n",
        "print(\"‚úÖ LightGBM model trained successfully!\")\n",
        "\n",
        "# --- Evaluate Performance ---\n",
        "print(\"\\n--- LightGBM Model Performance ---\")\n",
        "y_pred_lgbm = lgbm_model.predict(X_test_scaled)\n",
        "mae_lgbm = mean_absolute_error(y_test, y_pred_lgbm)\n",
        "r2_lgbm = r2_score(y_test, y_pred_lgbm)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae_lgbm:.2f}\")\n",
        "print(f\"R-squared (R¬≤): {r2_lgbm:.2f}\")\n",
        "\n",
        "# --- Feature Importance ---\n",
        "lgbm_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': lgbm_model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop features (LightGBM):\")\n",
        "print(lgbm_importance_df.head())"
      ],
      "metadata": {
        "id": "h6LAeylUvnL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Load the Dataset ---\n",
        "# Make sure 'cur_water_data_cleaned.csv' is in the same directory as this script\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/cur_water_data_cleaned.csv')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'cur_water_data_cleaned.csv' not found. Please check the file path.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Define Features and Target ---\n",
        "# Select the numerical columns to be used as features for the model\n",
        "numerical_features = [\n",
        "    'latitude', 'longitude', 'pH', 'dissolvedoxygen',\n",
        "    'bod', 'cod', 'nitrate', 'FecalColiform', 'Year', 'Month'\n",
        "]\n",
        "# Define the target variable we want to predict\n",
        "target = 'wqi'\n",
        "\n",
        "# --- 3. Data Cleaning and Preprocessing ---\n",
        "# Convert feature columns to numeric, replacing non-numeric values with NaN\n",
        "for col in numerical_features:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Fill any missing (NaN) values with the mean of that column\n",
        "for col in numerical_features:\n",
        "    if df[col].isnull().any():\n",
        "        df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "# Clean the target column as well\n",
        "df[target] = pd.to_numeric(df[target], errors='coerce')\n",
        "if df[target].isnull().any():\n",
        "    df[target].fillna(df[target].mean(), inplace=True)\n",
        "\n",
        "print(\"Data cleaning and preprocessing complete.\")\n",
        "\n",
        "# --- 4. Prepare Data for Modeling ---\n",
        "# Assign the features to X and the target to y\n",
        "X = df[numerical_features]\n",
        "y = df[target]\n",
        "\n",
        "# Split the data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 5. Train the RandomForestRegressor Model ---\n",
        "# Initialize the model with 100 trees for a good balance of performance and speed\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 6. Evaluate the Model ---\n",
        "# Make predictions on the unseen test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the performance of the model\n",
        "print(\"\\n--- Model Performance ---\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R-squared (R¬≤): {r2:.3f}\")\n",
        "print(\"--------------------------\")\n",
        "\n",
        "\n",
        "# --- 7. Save the Trained Model ---\n",
        "# Save the model to a file for later use\n",
        "model_filename = 'random_forest_regressor_model.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"\\nModel saved successfully as '{model_filename}'\")\n",
        "\n",
        "\n",
        "# --- Example of How to Load and Use the Model for a New Prediction ---\n",
        "print(\"\\n--- Prediction Example ---\")\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(model_filename)\n",
        "\n",
        "# Create a new data sample for prediction (example values)\n",
        "# The order of values must match the 'numerical_features' list\n",
        "new_sample = [[19.07, 72.87, 7.5, 5.0, 10.0, 150.0, 1.5, 400.0, 2024, 9]]\n",
        "new_prediction = loaded_model.predict(new_sample)\n",
        "\n",
        "print(f\"Predicted WQI for the new sample: {new_prediction[0]:.2f}\")\n",
        "print(\"--------------------------\")"
      ],
      "metadata": {
        "id": "Jj6oBgQWC5OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sarima forecasting\n"
      ],
      "metadata": {
        "id": "a6TzvTw3LN4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This command fixes the environment by reinstalling necessary libraries.\n",
        "# The verbose flag will show you the progress so you know it's not stuck.\n",
        "\n",
        "!pip uninstall -y pmdarima numpy statsmodels scipy\n",
        "!pip install --verbose pmdarima"
      ],
      "metadata": {
        "id": "LhmqI6F7V0UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "renVkPAobz5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import pmdarima as pm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "# --- 1. Load and Prepare the Data ---\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/cur_water_data_cleaned.csv')\n",
        "    df['sample_date'] = pd.to_datetime(df['sample_date'])\n",
        "    monthly_wqi = df.groupby('sample_date')['wqi'].mean().sort_index()\n",
        "    monthly_wqi = monthly_wqi.asfreq('MS', method='ffill')\n",
        "    print(\"Data prepared for time-series forecasting.\")\n",
        "    print(\"Latest WQI data point is for:\", monthly_wqi.index[-1].strftime('%B %Y'))\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'cur_water_data_cleaned.csv' not found. Please upload the file again.\")\n",
        "\n",
        "# --- 2. Find the Best Model and Train It (Faster Version) ---\n",
        "print(\"\\nFinding the best SARIMA model... You will now see the progress below:\")\n",
        "\n",
        "# The key change is trace=True, which shows you what the model is doing.\n",
        "# We also slightly reduce the search space (max_p, max_q) to speed it up.\n",
        "sarima_model = pm.auto_arima(monthly_wqi,\n",
        "                             start_p=1, start_q=1,\n",
        "                             test='adf',\n",
        "                             max_p=2, max_q=2, # Reduced search space\n",
        "                             m=12,\n",
        "                             d=None,\n",
        "                             seasonal=True,\n",
        "                             start_P=0,\n",
        "                             D=1,\n",
        "                             trace=True,       # <-- THIS IS THE IMPORTANT CHANGE\n",
        "                             error_action='ignore',\n",
        "                             suppress_warnings=True,\n",
        "                             stepwise=True)\n",
        "\n",
        "print(\"\\nBest SARIMA model found and trained.\")\n",
        "print(sarima_model.summary())\n",
        "\n",
        "# --- 3. Forecast and Visualize (No changes needed here) ---\n",
        "n_periods = 3\n",
        "forecast, conf_int = sarima_model.predict(n_periods=n_periods, return_conf_int=True)\n",
        "forecast_dates = pd.date_range(start=monthly_wqi.index[-1] + pd.DateOffset(months=1),\n",
        "                               periods=n_periods,\n",
        "                               freq='MS')\n",
        "forecast_series = pd.Series(forecast, index=forecast_dates)\n",
        "\n",
        "print(\"\\n--- Water Quality Index (WQI) Forecast ---\")\n",
        "for date, value in forecast_series.items():\n",
        "    print(f\"{date.strftime('%B %Y')}: {value:.2f}\")\n",
        "print(\"------------------------------------------\")\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(monthly_wqi.index, monthly_wqi, label='Historical WQI')\n",
        "plt.plot(forecast_series.index, forecast_series, label='Forecasted WQI', color='red', marker='o')\n",
        "plt.fill_between(forecast_series.index, conf_int[:, 0], conf_int[:, 1], color='red', alpha=0.1, label='95% Confidence Interval')\n",
        "plt.title('WQI Forecast for the Next 3 Months', fontsize=16)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Water Quality Index (WQI)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LPWtAmalD6m-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}